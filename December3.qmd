---
title: "December 3"
author: Shane Bateman
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
engine: python3
---



```{python}
import pandas as pd 
```


```{python}
import kagglehub

# Download latest version
path = kagglehub.dataset_download("khushikyad001/ai-impact-on-jobs-2030")

print("Path to dataset files:", path)
```


```{python}
import pandas as pd 
import numpy as np
from plotnine import *

print(os.listdir(path))

df = pd.read_csv(f"{path}\AI_Impact_on_Jobs_2030.csv")
```

```{python}

df.columns

```

```{python}

g1 = (
    ggplot(df, aes(x = 'Risk_Category', y = 'Years_Experience', fill = 'Education_Level')) 
    + geom_boxplot()
)

g1

```


```{python}

df['Risk_Category'].value_counts()

df['Risk_Category'] = df['Risk_Category'].map({'Low' : 1, 'Medium' : 2, 'High' : 3}).astype('float32')

df['Education_Level'] = df['Education_Level'].map({'High School' : 1, "Bachelor's" : 2, "Master's" : 3, "PhD" : 4}).astype('float32')

df['Risk_Categories'] = df['Risk_Category']

df = df.drop(columns = ['Risk_Category'])

length = (len(df)*.75)

train = df.loc[:length]
test = df.loc[length:]

x_train = train.iloc[2:-2,]
y_train = train.columns[-1:]



```


```{python}


# # #XGBoost Tuning to get the best parameters
# # # DO NOT RUN THIS BLOCK OF CODE USE THE SAVED VARIABLES 

# # params = {
# #     "objective": "binary:logistic",   # Set objective
# #     "eta": 0.1,                       # Set learning rate
# #     "eval_metric": ["auc", "error"],  # Track both AUC and error
# #     "tree_method": "hist",
# #     "seed": 111111,
# #     "nthread": 1,                     # Parallel threads
# # }

# # # Run CV inside XGBoost
# # cv_res = xgb.cv(
# #     params=params,
# #     dtrain=dtrain_w,              # Training data (DMatrix)
# #     num_boost_round=400,       # Number of rounds
# #     nfold=5,                    # 5-fold CV
# #     verbose_eval=20,            # Print every 20 iters
# #     stratified=True,            # Good practice for classification
# #     shuffle=True,
# # )

# # cv_res

# # # Identify best iteration
# # #Remove idmix if you wan tot
# # best_idx = cv_res['test-error-mean'].idxmin()
# # best_iter = int(best_idx) + 1 # Increment by 1 to get iteration
# # best_err  = float(cv_res.loc[best_idx, 'test-error-mean']) # Extract test error
# # best_auc  = float(cv_res.loc[best_idx, 'test-auc-mean']) if 'test-auc-mean' in cv_res.columns else np.nan # Extract test AUC

# # # Print results
# # print(f"Best iteration (by min test error): {best_iter}")
# # print(f"Min test error at best iter: {best_err:.6f}")
# # if not np.isnan(best_auc):
# #     print(f"Test AUC at best iter: {best_auc:.6f}")

# # # Set range of parameter values to try
# # grid = {
# #     "max_depth": [3,  7, 10],
# #     "min_child_weight": [ 5, 7, 10],
# # }
# # param_grid = list(ParameterGrid(grid))

# # # Set base model parameters
# # base_params = {
# #     "objective": "binary:logistic",
# #     "eta": 0.10,
# #     "eval_metric": ["error", "auc"],
# #     "tree_method": "hist",
# #     "seed": 111111,
# #     "nthread": 1,  # keep each worker single-threaded to avoid oversubscription
# # }


# # def run_one_cv(md, mcw):
# #     """Run xgb.cv for a single (max_depth, min_child_weight) pair and return best metrics."""
# #     params = base_params.copy()
# #     params.update({"max_depth": int(md), "min_child_weight": int(mcw)})

# #     cv = xgb.cv(
# #         params=params,
# #         dtrain=dtrain_w,              # DMatrix from earlier
# #         num_boost_round=1000,        # nrounds = 100
# #         nfold=5,                    # 5-fold CV
# #         early_stopping_rounds=20,   # stop if no improvement
# #         stratified=True,
# #         shuffle=True,
# #         verbose_eval=False,
# #         seed=111111,
# #     )

# #     # Best round is the length of the early-stopped trace
# #     best_round = len(cv)

# #     # Read AUC & error at the best round row explicitly
# #     best_row = cv.iloc[best_round - 1]
# #     best_err = float(best_row["test-error-mean"])
# #     best_auc = float(best_row["test-auc-mean"])

# #     # Return results
# #     return {
# #         "max_depth": md,
# #         "min_child_weight": mcw,
# #         "best_round": best_round,
# #         "test_error": best_err,
# #         "test_auc": best_auc,
# #     }

# # results = []
# # for p in tqdm(param_grid, desc="Grid CV (serial)"):
# #     results.append(run_one_cv(p["max_depth"], p["min_child_weight"]))



# # # Create and sort results data frame
# # cv_results_df = (
# #     pd.DataFrame(results)
# #       .sort_values(["test_error", "test_auc"], ascending=[True, False])
# #       .reset_index(drop=True)
# # )

# # # Identify best parameters0
# # best_pair = cv_results_df.iloc[0].to_dict()
# # print("Best (by min test_error, then max AUC):", best_pair)    


# # # Create results data frame
# # res_db = (
# #     cv_results_df[["max_depth", "min_child_weight", "test_auc", "test_error"]]
# #     .rename(columns={"test_auc": "auc", "test_error": "error"})
# #     .copy()
# # )

# # tuned_max_depth = int(best_pair['max_depth']) # Extract max depth
# # tuned_min_child = int(best_pair['min_child_weight']) # Extract min_child_weight

# # # Define gamma grid
# # gamma_vals = [0.00, 0.05, 0.10, 0.15, 0.20]

# # # Set base parameters
# # base_params = {
# #     "objective": "binary:logistic",
# #     "eta": 0.10,
# #     "max_depth": tuned_max_depth, # Uuse tuned value of max depth
# #     "min_child_weight": tuned_min_child, # Use tuned value of min child weight
# #     "tree_method": "hist",
# #     "eval_metric": ["auc", "error"],
# #     "seed": 111111,
# #     "nthread": 1,
# # }

# # ### Be careful this can take a long time to run ###
# # rows = [] # Create data frame to store valus
# # for g in tqdm(gamma_vals, desc="Gamma CV (serial)"): # For each gamma value
# #     params = base_params.copy() # Create copy of base parameters
# #     params["gamma"] = float(g) # Replace value with current gamma value

# #     # Run xgb.cv
# #     cv = xgb.cv(
# #         params=params,
# #         dtrain=dtrain_w,                # Set training data
# #         num_boost_round=1000,          # Set number of rounds
# #         nfold=5,                      # Set folds for cross validation
# #         early_stopping_rounds=20,     # Set early stopping rounds
# #         stratified=True,
# #         shuffle=True,
# #         verbose_eval=False,
# #         seed=111111, #Set seed
# #     )

# #     # Best iteration is the length of the early-stopped trace
# #     best_round = len(cv)
# #     best_row = cv.iloc[best_round - 1]
# #     # Store results from current iteration
# #     rows.append({
# #         "gamma": g,
# #         "best_round": int(best_round),
# #         "test_auc": float(best_row["test-auc-mean"]),
# #         "test_error": float(best_row["test-error-mean"]),
# #     })

# # # Join results into data frame
# # gamma_results = (pd.DataFrame(rows)
# #                    .sort_values(['test_error', 'test_auc'], ascending=[True, False])
# #                    .reset_index(drop=True))
# # # # View results
# # # display(gamma_results)
# # # Extract best value
# # best_gamma = float(gamma_results.iloc[0]['gamma'])
# # # Print out vest value
# # print(f"Selected gamma (by min test_error, then max AUC): {best_gamma:.2f}")

# # tuned_gamma = float(best_gamma) # Extract best gamma value


# # # Create grid of possible values
# # grid = {
# #     "subsample":        [0.6, 0.7, 0.8, 0.9, 1.0],
# #     "colsample_bytree": [0.6, 0.7, 0.8, 0.9, 1.0],
# # }
# # # Convert into parameter grid and then list
# # param_grid = list(ParameterGrid(grid))
# # # Set base parameters
# # base_params = {
# #     "objective": "binary:logistic",
# #     "eta": 0.10,
# #     "max_depth": tuned_max_depth, # Use tuned value for max depth
# #     "min_child_weight": tuned_min_child, # Use tuned value for min child weight
# #     "gamma": tuned_gamma, # Use tuned value for gamma
# #     "tree_method": "hist",
# #     "eval_metric": ["auc", "error"],
# #     "seed": 111111,
# #     "nthread": 1,                     # single core
# # }

# # # Create function
# # def run_one_cv(subsample, colsample_bytree):
# #     """Run xgb.cv for a single (subsample, colsample_bytree) and return best metrics."""
# #     params = base_params.copy() # Create copy of base parameters
# #     params.update({ # Update with values of subsample and colsample_bytree
# #         "subsample": float(subsample),
# #         "colsample_bytree": float(colsample_bytree),
# #     })
# #     # Run xgb.cv
# #     cv = xgb.cv(
# #         params=params,  # Set parameters
# #         dtrain=dtrain_w,  # Set training data
# #         num_boost_round=1000, # Set number of rounds
# #         nfold=5,  # Set cross-validation folds
# #         early_stopping_rounds=20, # Set number of early stopping rounds
# #         stratified=True,
# #         shuffle=True,
# #         verbose_eval=False,
# #         seed=111111, # Set seed
# #     )

# #     best_round = len(cv)             # early-stopped length
# #     best_row = cv.iloc[best_round - 1] # Identify best row
# #     # Return results
# #     return {
# #         "subsample": subsample,
# #         "colsample_bytree": colsample_bytree,
# #         "best_round": int(best_round),
# #         "test_auc": float(best_row["test-auc-mean"]),
# #         "test_error": float(best_row["test-error-mean"]),
# #     }

# # ### Be careful this can take a long time to run ###
# # rows = [] # Create empty list to store results
# # # For each set of parameters
# # for p in tqdm(param_grid, desc="Subsample × Colsample_bytree CV (serial)"):
# #     rows.append(run_one_cv(p["subsample"], p["colsample_bytree"])) # Run tuning function and store results
# # # Convert results into data frame
# # sc_results = (pd.DataFrame(rows)
# #                 .sort_values(['test_error','test_auc'], ascending=[True, False])
# #                 .reset_index(drop=True))
# # # # View results
# # # display(sc_results.head(10))
# # # Identify best results
# # best_sc = sc_results.iloc[0].to_dict()
# # # Store best results
# # print(f"Selected subsample={best_sc['subsample']}, "
# #       f"colsample_bytree={best_sc['colsample_bytree']} "
# #       f"(min test_error={best_sc['test_error']:.6f}, AUC={best_sc['test_auc']:.6f}, "
# #       f"best_round={best_sc['best_round']})")

# # tuned_subsample = float(best_sc['subsample']) # Extract best subsample
# # tuned_colsample = float(best_sc['colsample_bytree']) # Extract best colsample_bytree

# # # Set ETA values to try
# # etas = [0.3, 0.1, 0.05, 0.01, 0.005]
# # # Set base parameters
# # base_params = {
# #     "objective": "binary:logistic",
# #     "eval_metric": ["auc", "error"],
# #     "max_depth": tuned_max_depth, # Use tuned value for max depth
# #     "min_child_weight": tuned_min_child, # Use tuned value for min_child_weight
# #     "gamma": tuned_gamma, # Use tuned value for gamma
# #     "subsample": tuned_subsample, # Use tuned value for subsample
# #     "colsample_bytree": tuned_colsample, # Use tuned value for colsample_bytree
# #     "tree_method": "hist",
# #     "seed": 111111,
# #     "nthread": 1,                  # single core
# # }

# # ### Be careful this can take a long time to run ###
# # curves = []     # per-iteration logs for plotting
# # summaries = []  # one row per eta
# # # For each learning rate
# # for eta in tqdm(etas, desc="Learning-rate CV (serial)"):
# #     params = base_params.copy() # Create copy of parmameters
# #     params["eta"] = float(eta) # Update ETA value
# #     # Apply xgb.cv
# #     cv = xgb.cv(
# #         params=params, # Set parameters
# #         dtrain=dtrain_w, # Set training data
# #         num_boost_round=1000,  # run to 1000 unless ES stops early
# #         nfold=5, # Set folds for cross validation
# #         early_stopping_rounds=20, # Set early stopping rounds
# #         stratified=True,
# #         shuffle=True,
# #         verbose_eval=False,
# #         seed=111111,
# #     )

# #     # Extract data for model performance
# #     df_log = cv.reset_index().rename(columns={"index": "iter"})
# #     df_log["iter"] = df_log["iter"] + 1 # Increment iterations to get real number
# #     # fix hyphenated column names for plotnine
# #     df_log = df_log.rename(columns=lambda c: c.replace("-", "_"))
# #     df_log["eta"] = str(eta) # Store ETA value as a string
# #     curves.append(df_log) # Add values to data store

# #     # Identify best iteration
# #     best_round = len(cv)
# #     best_row = cv.iloc[best_round - 1] # Identify best row


# #     best_err = float(best_row["test-error-mean"]) # Extract best error value
# #     best_auc = float(best_row["test-auc-mean"]) # Extract best AUC value
# #     # Store results
# #     summaries.append({"eta": eta, "best_round": best_round, "test_error": best_err, "test_auc": best_auc})

# # # Combine curve data
# # curves_df = pd.concat(curves, ignore_index=True)
# # # Create data frame of result data
# # summ_df = pd.DataFrame(summaries).sort_values(
# #     ["test_error","test_auc"] ,
# #     ascending=[True, False]
# # ).reset_index(drop=True)

# # best_eta = float(summ_df.iloc[0]["eta"]) # Extract best learning rate
# # best_round = int(summ_df.iloc[0]["best_round"]) # Extract best round
# # print(f"Selected eta={best_eta} with best_round={best_round}, " # Print results
# #       f"test_error={summ_df.iloc[0]['test_error']:.6f}, "
# #       f"AUC={summ_df.iloc[0]['test_auc']:.6f}")

# # tuned_eta = float(best_eta) # Extract best learning rate



# # print(
# #     f"\nFinal tuned hyperparameters:\n"
# #     f"  max_depth        = {tuned_max_depth}\n"
# #     f"  min_child_weight = {tuned_min_child}\n"
# #     f"  gamma            = {tuned_gamma}\n"
# #     f"  subsample        = {tuned_subsample}\n"
# #     f"  colsample_bytree = {tuned_colsample}\n"
# #     f"  eta              = {tuned_eta}\n"
# #     f"  best_round       = {best_round}"
# # )













# # Best Parameters & Weighted
# # DO NOT RUN THE TUNING PARAMETERS IT TAKES AROUND 1 HOUR ON CRC

# max_depth        = 10
# min_child_weight = 5
# gamma            = 0.0
# subsample        = 0.6
# colsample_bytree = 0.6
# eta              = 0.3
# best_round       = 5








# # Count values
# counts = pd.Series(y_train).value_counts().sort_index()
# neg = int(counts.get(0, 0)); pos = int(counts.get(1, 0)) # Calculate positive and negative samples
# print(f"Number of negative samples: {neg}")
# print(f"Number of positive samples: {pos}")

# # Calculate ratio
# ratio = neg / pos
# # Set ratio as weight for positive samples
# w_tr = np.where(y_train == 1, ratio, 1.0).astype(np.float32)

# # Build weighted DMatrices
# dtrain_w = xgb.DMatrix(X_train.values, label=y_train, weight=w_tr)

# params = {
#     "objective": "binary:logistic",
#     "eval_metric": ["auc", "error"],
#     "max_depth": max_depth, # Use tuned value for max depth
#     "min_child_weight": min_child_weight, # Use tuned value for min_child_weight
#     "gamma": gamma, # Use tuned value for gamma
#     "subsample": subsample, # Use tuned value for subsample
#     "colsample_bytree": colsample_bytree, # Use tuned value for colsample_bytree
#     "eta": eta, # Use tuned value for eta
#     "tree_method": "hist",
#     "seed": 111111,
#     "nthread": 1,                  # single core
# }

# num_boost_round = best_round # Set number of rounds

# watchlist = [(dtrain_w, "train")] # Set data for evaluation
# xgb_tuned = xgb.train(params, # Set parameters
#                     dtrain_w,  # Set training data
#                     num_boost_round=num_boost_round, # Set number of rounds
#                     evals=watchlist,  # Set data to evaluate on
#                     verbose_eval=50) # Set print out frequency

# test_pred_w = xgb_tuned.predict(dtest) # Create predictions


# # Convert predictions into classes at 0.5
# test_pred_cls_w = (test_pred_w >= 0.5).astype(int)


# print("\nConfusion matrix:")
# cm = (confusion_matrix(y_test, test_pred_cls_w))
# disp = ConfusionMatrixDisplay(confusion_matrix=cm)# Set class labels
# disp.plot(cmap="Blues") # Set color map
# plt.title("Confusion Matrix — Weighted XGBoost") # Set title
# plt.savefig("confusion_matrix_weighted_and_tuned.png", dpi=300, bbox_inches="tight")
# plt.close()
# print("\nAccuracy):")
# print(accuracy_score(y_test, test_pred_cls_w)) # Get Accuracy


# # SHAP values to see importance of each column in the weighted tuned XGBoost model

# # Create TreeExplainer and compute SHAP values
# explainer = shap.TreeExplainer(xgb_tuned)
# shap_values = explainer(X_train)

# plt.figure()  # start a clean figure
# shap.plots.bar(shap_values, max_display=10)
# plt.title("Top 10 SHAP Feature Importances")   # optional title
# plt.savefig("shap_bar_weighted_and_tuned.png", dpi=300, bbox_inches="tight")
# plt.close()

# # Create and save beeswarm plot
# plt.figure(figsize=(10, 8))   # optional: wider figure
# shap.plots.beeswarm(shap_values, max_display=25)
# plt.title("SHAP Beeswarm — Weighted & Tuned XGBoost")  # optional title
# plt.savefig("shap_beeswarm_weighted_and_tuned.png", dpi=300, bbox_inches="tight")
# plt.close()









```



